{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "27a4956d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None) # will display full text in row\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import os, re, string\n",
    "from time import time\n",
    "from clean_text import clean_text\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523cbb4a",
   "metadata": {},
   "source": [
    "## Get Covid Tweet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "91714775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41157, 6)\n",
      "(3798, 6)\n"
     ]
    }
   ],
   "source": [
    "# Download data from https://www.kaggle.com/datatattle/covid-19-nlp-text-classification?select=Corona_NLP_train.csv\n",
    "train = pd.read_csv('../datasets/Corona_NLP_train.csv', encoding='ISO-8859-1')\n",
    "test = pd.read_csv('../datasets/Corona_NLP_test.csv', encoding='ISO-8859-1')\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702840b9",
   "metadata": {},
   "source": [
    "### Combine train and test, since we're using an unsupervised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "937a57eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train, test], axis=0)\n",
    "del train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c9f2856b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict to useful columns\n",
    "data = data[['Location', 'TweetAt', 'OriginalTweet']].rename(columns={'TweetAt': 'Date', 'OriginalTweet': 'Tweet'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e34362e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data for good measure\n",
    "data = data.sample(frac=0.01, random_state=123).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1ef7eca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>Date</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>London, UK</td>\n",
       "      <td>22-03-2020</td>\n",
       "      <td>QPAY cuts POS prices by 50% to help Qatari SMEs fight CoVid-19: QPAY International, a member of the NEXXO Network, the leading financial technology (Fintech) company in Qatar servicing over 15,000 Qatari SmallÂ ... https://t.co/ufT6VKKXNd #fintech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>18-03-2020</td>\n",
       "      <td>According to a Mariano's employee who just got out of work and on my bus, a fight broke out in the grocery store, glass was broken. Stay safe y'all. #coronavirus #COVID19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>London</td>\n",
       "      <td>08-04-2020</td>\n",
       "      <td>Just had a text that reads:  \\r\\r\\n\\r\\r\\nCONGRATULATIONS. YOU ARE NOW CLEAR TO LEAVE YOUR HOME AT ANY TIME AND LICK SUPERMARKET TROLLEY HANDLES. \\r\\r\\n\\r\\r\\nREGARDS BORIS JOHNSON\\r\\r\\n\\r\\r\\nPretty sure itÂs from my first wife. #LOCKDOWN #coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The World</td>\n",
       "      <td>25-03-2020</td>\n",
       "      <td>ArenÂt the prices of some vital products @costco extremely high? @USDAFoodSafety @USATODAY @LACountyDCBA @MayorOfLA @LANow  #food #socal #coronavirus #COVID2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Auckland, New Zealand</td>\n",
       "      <td>22-03-2020</td>\n",
       "      <td>Security guards at the supermarket. WTF is wrong with people that requires guards to protect the poor checkout staff #Covid_19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Location        Date  \\\n",
       "0             London, UK  22-03-2020   \n",
       "1            Chicago, IL  18-03-2020   \n",
       "2                 London  08-04-2020   \n",
       "3              The World  25-03-2020   \n",
       "4  Auckland, New Zealand  22-03-2020   \n",
       "\n",
       "                                                                                                                                                                                                                                                        Tweet  \n",
       "0     QPAY cuts POS prices by 50% to help Qatari SMEs fight CoVid-19: QPAY International, a member of the NEXXO Network, the leading financial technology (Fintech) company in Qatar servicing over 15,000 Qatari SmallÂ ... https://t.co/ufT6VKKXNd #fintech  \n",
       "1                                                                                  According to a Mariano's employee who just got out of work and on my bus, a fight broke out in the grocery store, glass was broken. Stay safe y'all. #coronavirus #COVID19  \n",
       "2  Just had a text that reads:  \\r\\r\\n\\r\\r\\nCONGRATULATIONS. YOU ARE NOW CLEAR TO LEAVE YOUR HOME AT ANY TIME AND LICK SUPERMARKET TROLLEY HANDLES. \\r\\r\\n\\r\\r\\nREGARDS BORIS JOHNSON\\r\\r\\n\\r\\r\\nPretty sure itÂs from my first wife. #LOCKDOWN #coronavirus  \n",
       "3                                                                                           ArenÂt the prices of some vital products @costco extremely high? @USDAFoodSafety @USATODAY @LACountyDCBA @MayorOfLA @LANow  #food #socal #coronavirus #COVID2019  \n",
       "4                                                                                                                              Security guards at the supermarket. WTF is wrong with people that requires guards to protect the poor checkout staff #Covid_19  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "55cd47be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(450, 3)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171f0e9d",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0f819b",
   "metadata": {},
   "source": [
    "### Fix tweets\n",
    "I remove html, URLs, punctuation, hashtags, emoticons, convert contractions, remove stop words, lematize words, and convert everything to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c12a3479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizd (somewhat) version of word `coronavirus`\n",
    "def standardize_words(string):\n",
    "    string = string.replace('COVID-19', 'coronavirus')\n",
    "    string = string.replace(\"COVID 19\" , 'coronavirus')\n",
    "    string = string.replace(\"Covid-19\", 'coronavirus')\n",
    "    string = string.replace('COVID?19', 'coronavirus')\n",
    "    string = string.replace('covid', 'coronavirus')\n",
    "    string = string.replace('COVID', 'coronavirus')\n",
    "    string = string.replace('Covid_19', 'coronavirus')\n",
    "    string = string.replace('COVID2019', 'coronavirus')\n",
    "    string = string.replace('coronavirus19', 'coronavirus')\n",
    "    string = string.replace('COVID', 'coronavirus')\n",
    "    string = string.replace('covid', 'coronavirus')\n",
    "    string = string.replace('tp', 'toilet paper')\n",
    "    string = string.replace('TP', 'toilet paper')\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b8608d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = pd.DataFrame(data['Tweet'].apply(lambda x: standardize_words(x)).rename('clean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "09df2d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 3.0 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "processed = pd.DataFrame(clean_text().run(processed['clean'], no_stop_words=True, \n",
    "                                          remove_punctuation=True, lemmatize=True).rename('clean'))\n",
    "print('Total time:', round(time() - start, 0), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "536371fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qpay cut po price fifty help qatari smes fight covid nineteen qpay international member nexxo network lead financial technology fintech company qatar service zero qatari small httoilet paper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>accord mariano employee get work bus fight broke grocery store glass broken stay safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text read congratulation clear leave home time lick supermarket trolley handle regard boris johnson pretty sure first wife</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>price vital product extremely high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>security guard supermarket wtf wrong people require guard protect poor checkout staff</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                             clean\n",
       "0  qpay cut po price fifty help qatari smes fight covid nineteen qpay international member nexxo network lead financial technology fintech company qatar service zero qatari small httoilet paper \n",
       "1                                                                                                            accord mariano employee get work bus fight broke grocery store glass broken stay safe\n",
       "2                                                                       text read congratulation clear leave home time lick supermarket trolley handle regard boris johnson pretty sure first wife\n",
       "3                                                                                                                                                               price vital product extremely high\n",
       "4                                                                                                            security guard supermarket wtf wrong people require guard protect poor checkout staff"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5a58ee",
   "metadata": {},
   "source": [
    "#### Distribution of word frequencies in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a3243a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_freq(series):\n",
    "    '''\n",
    "    Counts word frequency across all documents (rows) in a pd.Series.\n",
    "    :param series: pd.Series\n",
    "    :returns: pd.DataFrame, where index is each unique word in corpus and column is count of\n",
    "        the occurrence of that word across all documents.\n",
    "    '''\n",
    "    temp = [i.split() for i in series] \n",
    "    freq = defaultdict(int)  # Get freq of each word across all documents\n",
    "    for indiv_doc in temp:\n",
    "        for token in indiv_doc:\n",
    "            freq[token] += 1\n",
    "        \n",
    "    word_freq = pd.DataFrame.from_dict(freq, orient='index')\\\n",
    "        .reset_index().rename(columns={'index': 'word', 0: 'freq'})\n",
    "    return word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "43ec5a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = count_word_freq(processed['clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2b37595d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    2516.000000\n",
      "mean        2.955087\n",
      "std         6.839888\n",
      "min         1.000000\n",
      "25%         1.000000\n",
      "50%         1.000000\n",
      "75%         2.000000\n",
      "max       141.000000\n",
      "Name: freq, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(word_freq['freq'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8320927c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words that appear only once in corpus: 1529\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of words that appear only once in corpus: {len(word_freq[word_freq['freq']==1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a8c0ab22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some examples of rare words:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2                po\n",
       "7              smes\n",
       "11    international\n",
       "13            nexxo\n",
       "14          network\n",
       "17       technology\n",
       "18          fintech\n",
       "20            qatar\n",
       "25          mariano\n",
       "30            broke\n",
       "Name: word, dtype: object"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Some examples of rare words:')\n",
    "word_freq[word_freq['freq']==1]['word'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8a4dd9-07cc-497b-a183-2a5796ec2a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18873488",
   "metadata": {},
   "source": [
    "### Fix date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f4d8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed['Date'] = pd.to_datetime(data['Date'], format='%d-%m-%Y', errors='coerce').dt.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8984bec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processed['Date'].isnull().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9c552f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date range\n",
    "for i in sorted(processed['Date'].unique()):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba00b34c",
   "metadata": {},
   "source": [
    "### Fix location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcc2a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['Location'].nunique())\n",
    "print(data['Location'].isnull().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41861df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Location'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae029e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = data['Location'].str.replace(\"[^a-zA-Z ]\", '', regex=True)\n",
    "foo = foo.replace(np.NaN, '') # convert missings to empty strings\n",
    "foo = foo.apply(lambda x: ' '.join(x.split())) # remove excess whitespace from some rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d15daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = pd.read_csv('../datasets/US States.csv')\n",
    "print(states.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0751047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "states_dict = dict(zip(states['Abbreviation'].tolist(), states['State'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77963c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookup function to replace state abbreviations with state names\n",
    "def lookup_replace(col, dict_map):\n",
    "    '''\n",
    "    '''\n",
    "    new = []\n",
    "    for i in range(len(col)):\n",
    "        tmp = []\n",
    "        try:\n",
    "            for word in col.iloc[i].split():\n",
    "                if word in dict_map.keys():\n",
    "                    name = dict_map[word]\n",
    "                else:\n",
    "                    name = word\n",
    "                tmp.append(name)\n",
    "        except AttributeError:\n",
    "            tmp = np.NaN\n",
    "        new.append(tmp)\n",
    "    return pd.Series(new).apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0963c361",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = lookup_replace(foo, states_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f410746",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo.sample(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca69666",
   "metadata": {},
   "source": [
    "## Keywords using pretrained BERT model and cosine similarity\n",
    "\n",
    "[Credit](https://towardsdatascience.com/keyword-extraction-with-bert-724efca412ea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7daa35b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "12b7ce5c-5b7f-4c31-84b2-cd30ea2cfe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_keywords(col, n_keywords=5, ngram_range=(1, 1)):\n",
    "    '''\n",
    "    Uses pretrained BERT model from sentence-transformer to generate nth keywords per document.\n",
    "    :param col:, pd.Series, input (cleaned) text, one document per row\n",
    "    :param n_keywords: int, number of top keywords per document to return.\n",
    "    :param ngram_range: tuple, ngram range, e.g. (1,1) is monogram, (2,2) is bigram.\n",
    "    :returns: list of lists, where each sublist contains top keywords per document\n",
    "    '''\n",
    "    keyword_list = []\n",
    "    for i in range(len(processed)):\n",
    "        if (i % 200==0 and i > 0):\n",
    "            print(i)\n",
    "        try:\n",
    "            count = CountVectorizer(ngram_range=ngram_range).fit(processed['clean'].iloc[i:i+1].tolist()) # pd.Series per row/document\n",
    "            candidates = count.get_feature_names()\n",
    "            doc_embedding = model.encode(processed['clean'].iloc[i:i+1].tolist())\n",
    "            candidate_embeddings = model.encode(candidates)\n",
    "\n",
    "            # Get keywords using cosine similarity\n",
    "            top_n = n_keywords\n",
    "            distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "            keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
    "        except ValueError: # empty string, only stop words or not intelligible\n",
    "            keywords = [np.NaN]\n",
    "        keyword_list.append(keywords)\n",
    "    return keyword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "14f0ee1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "400\n",
      "Total time: 57.0 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "keywords = bert_keywords(processed['clean'])\n",
    "print('Total time:', round(time() - start, 0), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b19b801e-c38e-421c-a280-42cb0257028e",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed['bert_keywords'] = pd.Series(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7113dd87-2051-4fef-8a29-e0f44c24f628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean</th>\n",
       "      <th>bert_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>bi bulletin five deal recent turbulence experienced emerge market economy bi org</td>\n",
       "      <td>[market, emerge, five, turbulence, economy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>go supermarket emerald busiest see live area much cheese except specialty cheese pasta tin tomato flour tp tissue run kitty litter</td>\n",
       "      <td>[flour, tomato, supermarket, pasta, cheese]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>challenge uncertain time crucial vermonter protect exploitative scam consumer abuse learn coronavirus related scam currently circulation respond</td>\n",
       "      <td>[exploitative, abuse, vermonter, scam, coronavirus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>coronavirus antidote home test toilet paper sanitizers mask movie myth amp tip gt gt gt coronavirus</td>\n",
       "      <td>[sanitizers, antidote, movie, toilet, coronavirus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>forget thing stock tea live without morning cuppa coronavirus</td>\n",
       "      <td>[stock, without, forget, coronavirus, tea]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                clean  \\\n",
       "445                                                                  bi bulletin five deal recent turbulence experienced emerge market economy bi org   \n",
       "446                go supermarket emerald busiest see live area much cheese except specialty cheese pasta tin tomato flour tp tissue run kitty litter   \n",
       "447  challenge uncertain time crucial vermonter protect exploitative scam consumer abuse learn coronavirus related scam currently circulation respond   \n",
       "448                                               coronavirus antidote home test toilet paper sanitizers mask movie myth amp tip gt gt gt coronavirus   \n",
       "449                                                                                     forget thing stock tea live without morning cuppa coronavirus   \n",
       "\n",
       "                                           bert_keywords  \n",
       "445          [market, emerge, five, turbulence, economy]  \n",
       "446          [flour, tomato, supermarket, pasta, cheese]  \n",
       "447  [exploitative, abuse, vermonter, scam, coronavirus]  \n",
       "448   [sanitizers, antidote, movie, toilet, coronavirus]  \n",
       "449           [stock, without, forget, coronavirus, tea]  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "21643fa0-c9e8-4266-8ffb-95764405f92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "400\n",
      "Total time: 77.0 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "keywords = bert_keywords(processed['clean'], ngram_range=(2, 2))\n",
    "print('Total time:', round(time() - start, 0), 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc8edf5-464b-4ea9-8b5b-88839e5961a4",
   "metadata": {},
   "source": [
    "## Keyword comparison to TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13fefb0",
   "metadata": {
    "tags": []
   },
   "source": [
    "[Term frequency-inverse document frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) is a statistic reflecting a given word's importance to a particular document in a collection of documents (i.e. a corpus). This statistic is bounded between 0 and 1, with higher scores indicating a given word is comparably rarer (i.e. more salient) in a particular document. This metric is commonly used to extract keywords about a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f47ce450",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a1c96d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 0.0 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "vectors = vectorizer.fit_transform(processed['clean'].tolist()) # scipy.sparse.csr.csr_matrix\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "print('Total time:', round(time() - start, 0), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5c4c6d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 0.0 seconds\n"
     ]
    }
   ],
   "source": [
    "# Lookup dict for each document's words\n",
    "# Note - sklearn's TfidfVectorizer smoothes the IDF function, resulting in fewer words per document than orig\n",
    "start = time()\n",
    "tfidf_dict = {}\n",
    "tfidf_values = []\n",
    "for doc in range(len(denselist)):\n",
    "    positions = [idx for idx, val in enumerate(denselist[doc]) if val > 0] # get index if word in document, i.e. > 0 tf-idf\n",
    "    values = [val for val in denselist[doc] if val > 0] # tf-idf values in doc\n",
    "    words = [feature_names[i] for i in positions] # words themselves\n",
    "    tfidf_dict[doc] = dict(zip(words, values))\n",
    "    tfidf_values += values # TF-IDF values for all words in all documents\n",
    "print('Total time:', round(time() - start, 0), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "39eec078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qatari': 0.40592678911401797,\n",
       " 'qpay': 0.40592678911401797,\n",
       " 'fintech': 0.20296339455700899,\n",
       " 'international': 0.20296339455700899,\n",
       " 'network': 0.20296339455700899}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 5 most important words according to TF-IDF of first document\n",
    "dict(sorted(tfidf_dict[0].items(), key=lambda item: item[1], reverse=True)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc676433",
   "metadata": {},
   "source": [
    "#### Restrict to top *n* keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c101f6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_keywords(dictionary, n=5):\n",
    "    '''\n",
    "    Restricts to top n most important words per document, returning this as pd.Series\n",
    "    :param dictionary: dict, tf-idf dictionary where each subdictionary pertains to an individual document\n",
    "    :param n: int, number of top keywords to limit to\n",
    "    :returns: pd.Series\n",
    "    '''\n",
    "    key_wds = []\n",
    "    for i in range(len(dictionary)):\n",
    "        key_wds.append(list(dict(sorted(dictionary[i].items(), key=lambda item: item[1], reverse=True)[:n]).keys()))\n",
    "    return pd.Series(key_wds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0f481dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed['tfidf_keywords'] = top_n_keywords(tfidf_dict, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a902a752-f7c9-43da-8ded-7bf4ca0295f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean</th>\n",
       "      <th>bert_keywords</th>\n",
       "      <th>tfidf_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qpay cut po price fifty help qatari smes fight covid nineteen qpay international member nexxo network lead financial technology fintech company qatar service zero qatari small</td>\n",
       "      <td>[financial, qatar, qatari, fifty, fintech]</td>\n",
       "      <td>[qatari, qpay, fintech, international, network]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>accord mariano employee get work bus fight broke grocery store glass broken stay safe</td>\n",
       "      <td>[broken, fight, broke, grocery, bus]</td>\n",
       "      <td>[broke, broken, mariano, bus, glass]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text read congratulation clear leave home time lick supermarket trolley handle regard boris johnson pretty sure first wife</td>\n",
       "      <td>[home, congratulation, trolley, supermarket, wife]</td>\n",
       "      <td>[congratulation, pretty, text, boris, clear]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>price vital product extremely high</td>\n",
       "      <td>[product, price, extremely, vital, high]</td>\n",
       "      <td>[extremely, vital, product, high, price]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>security guard supermarket wtf wrong people require guard protect poor checkout staff</td>\n",
       "      <td>[poor, guard, security, checkout, supermarket]</td>\n",
       "      <td>[guard, checkout, wtf, poor, require]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                             clean  \\\n",
       "0  qpay cut po price fifty help qatari smes fight covid nineteen qpay international member nexxo network lead financial technology fintech company qatar service zero qatari small   \n",
       "1                                                                                            accord mariano employee get work bus fight broke grocery store glass broken stay safe   \n",
       "2                                                       text read congratulation clear leave home time lick supermarket trolley handle regard boris johnson pretty sure first wife   \n",
       "3                                                                                                                                               price vital product extremely high   \n",
       "4                                                                                            security guard supermarket wtf wrong people require guard protect poor checkout staff   \n",
       "\n",
       "                                        bert_keywords  \\\n",
       "0          [financial, qatar, qatari, fifty, fintech]   \n",
       "1                [broken, fight, broke, grocery, bus]   \n",
       "2  [home, congratulation, trolley, supermarket, wife]   \n",
       "3            [product, price, extremely, vital, high]   \n",
       "4      [poor, guard, security, checkout, supermarket]   \n",
       "\n",
       "                                    tfidf_keywords  \n",
       "0  [qatari, qpay, fintech, international, network]  \n",
       "1             [broke, broken, mariano, bus, glass]  \n",
       "2     [congratulation, pretty, text, boris, clear]  \n",
       "3         [extremely, vital, product, high, price]  \n",
       "4            [guard, checkout, wtf, poor, require]  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
