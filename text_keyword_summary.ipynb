{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27a4956d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None) # will display full text in row\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import os, re, string\n",
    "from time import time\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "#nltk.download(['punkt', 'stopwords', 'averaged_perceptron_tagger', 'wordnet'], quiet=True)\n",
    "from clean_text import clean_text\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523cbb4a",
   "metadata": {},
   "source": [
    "## Get Covid Tweet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7d5ef2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41157, 6)\n",
      "(3798, 6)\n"
     ]
    }
   ],
   "source": [
    "# Download data from https://www.kaggle.com/datatattle/covid-19-nlp-text-classification?select=Corona_NLP_train.csv\n",
    "train = pd.read_csv('../datasets/Corona_NLP_train.csv', encoding='ISO-8859-1')\n",
    "test = pd.read_csv('../datasets/Corona_NLP_test.csv', encoding='ISO-8859-1')\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5d8098",
   "metadata": {},
   "source": [
    "### Combine train and test, since we're using an unsupervised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0380a51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train, test], axis=0)\n",
    "del train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec940985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict to useful columns\n",
    "data = data[['Location', 'TweetAt', 'OriginalTweet']].rename(columns={'TweetAt': 'Date', 'OriginalTweet': 'Tweet'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "867609a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data for good measure\n",
    "data = data.sample(frac=1.0, random_state=999).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d33b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceedff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171f0e9d",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb116039",
   "metadata": {},
   "source": [
    "### Fix tweets\n",
    "I remove html, URLs, punctuation, hashtags, emoticons, convert contractions, lemmatize words, and convert everything to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95a2d707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizd (somewhat) version of word `coronavirus`\n",
    "def standardize_covid(string):\n",
    "    string = string.replace('COVID-19', 'coronavirus')\n",
    "    string = string.replace(\"COVID 19\" , 'coronavirus')\n",
    "    string = string.replace(\"Covid-19\", 'coronavirus')\n",
    "    string = string.replace('COVID?19', 'coronavirus')\n",
    "    string = string.replace('covid', 'coronavirus')\n",
    "    string = string.replace('COVID', 'coronavirus')\n",
    "    string = string.replace('Covid_19', 'coronavirus')\n",
    "    string = string.replace('COVID2019', 'coronavirus')\n",
    "    string = string.replace('coronavirus19', 'coronavirus')\n",
    "    string = string.replace('COVID', 'coronavirus')\n",
    "    string = string.replace('covid', 'coronavirus')\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4602b486",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = pd.DataFrame(data['Tweet'].apply(lambda x: standardize_covid(x)).rename('clean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a83deeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 179.0 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "processed = pd.DataFrame(clean_text().run(processed['clean'], no_stop_words=False, \n",
    "                                          remove_punctuation=True, lemmatize=True).rename('clean'))\n",
    "print('Total time:', round(time() - start, 0), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31b735c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yeah imagine that economy in much well shape not fudge data to hide ongoing degrowrth oil price well correlate with crude price and of course no pogrom against muslim in kashmir and elsewhere and respond to covid one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>side effect of hedge be cause massive cost for airline which be now oblige to pay oil price accord to pre agree contract despite not need almost any oil at all agree price be always way high than current price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thank you to those on the frontline of coronavirus the cleaner driver supermarket assistant and so many many more</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>distillery have switch portion of their production from alcohol to hand sanitizer to help with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i just come from the supermarket a line wspace betw customer wait to enter store the new normal our life compartmentalize human solidarity kill by psychology of social distance break the distance and you get curse oh it will stay</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                   clean\n",
       "0               yeah imagine that economy in much well shape not fudge data to hide ongoing degrowrth oil price well correlate with crude price and of course no pogrom against muslim in kashmir and elsewhere and respond to covid one\n",
       "1                      side effect of hedge be cause massive cost for airline which be now oblige to pay oil price accord to pre agree contract despite not need almost any oil at all agree price be always way high than current price\n",
       "2                                                                                                                      thank you to those on the frontline of coronavirus the cleaner driver supermarket assistant and so many many more\n",
       "3                                                                                                                                         distillery have switch portion of their production from alcohol to hand sanitizer to help with\n",
       "4  i just come from the supermarket a line wspace betw customer wait to enter store the new normal our life compartmentalize human solidarity kill by psychology of social distance break the distance and you get curse oh it will stay"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ae24f8",
   "metadata": {},
   "source": [
    "#### Remove words only present once in corpus (e.g. misspellings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9ef3308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_freq(series):\n",
    "    '''\n",
    "    Counts word frequency across all documents (rows) in a pd.Series.\n",
    "    :param series: pd.Series\n",
    "    :returns: pd.DataFrame, where index is each unique word in corpus and column is count of\n",
    "        the occurrence of that word across all documents.\n",
    "    '''\n",
    "    temp = [i.split() for i in series] \n",
    "    freq = defaultdict(int)  # Get freq of each word across all documents\n",
    "    for indiv_doc in temp:\n",
    "        for token in indiv_doc:\n",
    "            freq[token] += 1\n",
    "        \n",
    "    word_freq = pd.DataFrame.from_dict(freq, orient='index')\\\n",
    "        .reset_index().rename(columns={'index': 'word', 0: 'freq'})\n",
    "    return word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63233e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = count_word_freq(processed['clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7e04270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    31378.000000\n",
      "mean        40.534292\n",
      "std        619.113152\n",
      "min          1.000000\n",
      "25%          1.000000\n",
      "50%          1.000000\n",
      "75%          5.000000\n",
      "max      49109.000000\n",
      "Name: freq, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(word_freq['freq'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0c01fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words that appear only once in corpus: 16731\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of words that appear only once in corpus: {len(word_freq[word_freq['freq']==1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f878258c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some examples of rare words:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14               degrowrth\n",
       "24                  pogrom\n",
       "90                  wspace\n",
       "91                    betw\n",
       "100       compartmentalize\n",
       "171           asiegercares\n",
       "270                colleys\n",
       "304               usatoday\n",
       "307            mecklenburg\n",
       "334          centurytowers\n",
       "341                  recul\n",
       "345          consommateurs\n",
       "360              coincides\n",
       "384             wondrously\n",
       "447          lockdownghana\n",
       "487               tastiest\n",
       "488             healthiest\n",
       "639                  penal\n",
       "647    restartingrebooting\n",
       "668           aboutmissing\n",
       "Name: word, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Some examples of rare words:')\n",
    "word_freq[word_freq['freq']==1]['word'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4b1f60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed['clean'] = clean_text().remove_infreq_words(processed['clean'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c996c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14647.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>85.693589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>904.070711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>49109.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               freq\n",
       "count  14647.000000\n",
       "mean      85.693589\n",
       "std      904.070711\n",
       "min        2.000000\n",
       "25%        2.000000\n",
       "50%        5.000000\n",
       "75%       18.000000\n",
       "max    49109.000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_word_freq(processed['clean']).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e7ce18",
   "metadata": {},
   "source": [
    "### Fix date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcd20907",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed['Date'] = pd.to_datetime(data['Date'], format='%d-%m-%Y', errors='coerce').dt.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0b7ba94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(processed['Date'].isnull().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "733e1ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-02\n",
      "2020-03-03\n",
      "2020-03-04\n",
      "2020-03-05\n",
      "2020-03-06\n",
      "2020-03-07\n",
      "2020-03-08\n",
      "2020-03-09\n",
      "2020-03-10\n",
      "2020-03-11\n",
      "2020-03-12\n",
      "2020-03-13\n",
      "2020-03-14\n",
      "2020-03-15\n",
      "2020-03-16\n",
      "2020-03-17\n",
      "2020-03-18\n",
      "2020-03-19\n",
      "2020-03-20\n",
      "2020-03-21\n",
      "2020-03-22\n",
      "2020-03-23\n",
      "2020-03-24\n",
      "2020-03-25\n",
      "2020-03-26\n",
      "2020-03-27\n",
      "2020-03-28\n",
      "2020-03-29\n",
      "2020-03-30\n",
      "2020-03-31\n",
      "2020-04-01\n",
      "2020-04-02\n",
      "2020-04-03\n",
      "2020-04-04\n",
      "2020-04-05\n",
      "2020-04-06\n",
      "2020-04-07\n",
      "2020-04-08\n",
      "2020-04-09\n",
      "2020-04-10\n",
      "2020-04-11\n",
      "2020-04-12\n",
      "2020-04-13\n",
      "2020-04-14\n"
     ]
    }
   ],
   "source": [
    "# Date range\n",
    "for i in sorted(processed['Date'].unique()):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c052c65",
   "metadata": {},
   "source": [
    "### Fix location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a992d702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13127\n",
      "0.20963185407629853\n"
     ]
    }
   ],
   "source": [
    "print(data['Location'].nunique())\n",
    "print(data['Location'].isnull().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a0404734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    New Delhi, India\n",
       "1                   ?\n",
       "2                 NaN\n",
       "3            LES, NYC\n",
       "4                 NYC\n",
       "5                 NaN\n",
       "6                 NaN\n",
       "7            VA ?? MI\n",
       "8    Hyderabad, India\n",
       "9      Lagos, Nigeria\n",
       "Name: Location, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Location'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc298871",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = data['Location'].str.replace(\"[^a-zA-Z ]\", '', regex=True)\n",
    "foo = foo.replace(np.NaN, '') # convert missings to empty strings\n",
    "foo = foo.apply(lambda x: ' '.join(x.split())) # remove excess whitespace from some rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa8e6070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            State Abbreviation\n",
      "0         ALABAMA           AL\n",
      "1          ALASKA           AK\n",
      "2  AMERICAN SAMOA           AS\n",
      "3         ARIZONA           AZ\n",
      "4        ARKANSAS           AR\n"
     ]
    }
   ],
   "source": [
    "states = pd.read_csv('../datasets/US States.csv')\n",
    "print(states.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84101bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "states_dict = dict(zip(states['Abbreviation'].tolist(), states['State'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6c33ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookup function to replace state abbreviations with state names\n",
    "def lookup_replace(col, dict_map):\n",
    "    '''\n",
    "    '''\n",
    "    new = []\n",
    "    for i in range(len(col)):\n",
    "        tmp = []\n",
    "        try:\n",
    "            for word in col.iloc[i].split():\n",
    "                if word in dict_map.keys():\n",
    "                    name = dict_map[word]\n",
    "                else:\n",
    "                    name = word\n",
    "                tmp.append(name)\n",
    "        except AttributeError:\n",
    "            tmp = np.NaN\n",
    "        new.append(tmp)\n",
    "    return pd.Series(new).apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83cfabc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = lookup_replace(foo, states_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75a0345c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21716                          \n",
       "22832                          \n",
       "9239             United Kingdom\n",
       "41709        Brisbane Australia\n",
       "14625             Merton London\n",
       "16608               Los Angeles\n",
       "26339                 London UK\n",
       "14640              Izmir Turkey\n",
       "7951     Lower mainland Toronto\n",
       "18864                    London\n",
       "6559        Nashville TENNESSEE\n",
       "7084      Sacramento CALIFORNIA\n",
       "31427                          \n",
       "16566                Appalachia\n",
       "43380                          \n",
       "41403             Beijing China\n",
       "3083            Phoenix ARIZONA\n",
       "15739                          \n",
       "37662      Greer SOUTH CAROLINA\n",
       "26643                          \n",
       "dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo.sample(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779b5607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a5deb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064c7025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2249a30",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "[Term frequency-inverse document frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) is a statistic reflecting a given word's importance to a particular document in a collection of documents (i.e. a corpus). This statistic is bounded between 0 and 1, with higher scores indicating a given word is comparably rarer (i.e. more salient) in a particular document. This metric is commonly used to extract keywords about a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae19a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03612f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "vectors = vectorizer.fit_transform(processed['clean'].tolist()) # scipy.sparse.csr.csr_matrix\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "print('Total time:', round(time() - start, 0), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17ee40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookup dict for each document's words\n",
    "# Note - sklearn's TfidfVectorizer smoothes the IDF function, resulting in fewer words per document than orig\n",
    "start = time()\n",
    "tfidf_dict = {}\n",
    "tfidf_values = []\n",
    "for doc in range(len(denselist)):\n",
    "    positions = [idx for idx, val in enumerate(denselist[doc]) if val > 0] # get index if word in document, i.e. > 0 tf-idf\n",
    "    values = [val for val in denselist[doc] if val > 0] # tf-idf values in doc\n",
    "    words = [feature_names[i] for i in positions] # words themselves\n",
    "    tfidf_dict[doc] = dict(zip(words, values))\n",
    "    tfidf_values += values # TF-IDF values for all words in all documents\n",
    "print('Total time:', round(time() - start, 0), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa85569",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad6e811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 5 most important words according to TF-IDF of first document\n",
    "dict(sorted(tfidf_dict[0].items(), key=lambda item: item[1], reverse=True)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1428db",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tfidf_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750e6a03",
   "metadata": {},
   "source": [
    "### Distribution of TF-IDF values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e6c1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(tfidf_values)\n",
    "plt.xlabel('TF-IDF Value')\n",
    "plt.title(\"Distribution of TF-IDF Values in Coronavirus Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abaf1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(tfidf_values).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3232317c",
   "metadata": {},
   "source": [
    "### Restrict to top *n* keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdf8da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_keywords(dictionary, n=10):\n",
    "    '''\n",
    "    Restricts to top n most important words per document, returning this as pd.Series\n",
    "    :param dictionary: dict, tf-idf dictionary where each subdictionary pertains to an individual document\n",
    "    :param n: int, number of top keywords to limit to\n",
    "    :returns: pd.Series\n",
    "    '''\n",
    "    key_wds = []\n",
    "    for i in range(len(dictionary)):\n",
    "        key_wds.append(list(dict(sorted(dictionary[i].items(), key=lambda item: item[1], reverse=True)[:n]).keys()))\n",
    "    return pd.Series(key_wds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b187cb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed['tfidf_top10'] = top_n_keywords(tfidf_dict, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31220f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_values(dictionary, n=10):\n",
    "    '''\n",
    "    Gets TF-IDF values among top n most important words\n",
    "    :dictionary: dict, tf-idf dictionary where each \n",
    "    '''\n",
    "    vals = []\n",
    "    for i in range(len(dictionary)):\n",
    "        vals += list(dict(sorted(dictionary[i].items(), key=lambda item: item[1], reverse=True)[:n]).values())\n",
    "    return pd.Series(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f44d40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_values(tfidf_dict, n=10).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b66a59",
   "metadata": {},
   "source": [
    "### Filter top keywords using minimum TF-IDF score\n",
    "\n",
    "As indicated above, some of the top 10 keywords have low TF-IDF scores, meaning few of the words in a given review are meaningful according to this metric. To remove these, the function below defines keywords based on a percentile of the TF-IDF distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fb81c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_top_keywords(dictionary, filter_val=0.2, max_keywords=10):\n",
    "    filtered_words = []\n",
    "    for i in range(len(dictionary)):\n",
    "        words = [key for key, val in dictionary[i].items() if val > filter_val]\n",
    "        if len(words) == 0: # Take top words if none meet threshold\n",
    "            words = list(dict(sorted(dictionary[i].items(), key=lambda item: item[1], reverse=True)[:1]).keys())\n",
    "        words = words[:max_keywords]\n",
    "        filtered_words.append(words)\n",
    "    return pd.Series(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b5c943",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_val = np.percentile(pd.Series(tfidf_values), 90)\n",
    "processed['tfidf_90pct'] = filter_top_keywords(tfidf_dict, filter_val=filter_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc9c636",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed['tfidf_90pct'].apply(lambda x: len(x)).describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
